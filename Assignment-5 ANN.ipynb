{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network for Customer’s Churn Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooja Umathe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn’s prediction could be a great asset in the business strategy for retention applying before the exit of customers and We will create a real model with python, applied on a bank environment. This model will tell us if the customer is going or not to exit from the bank. Churn rate (sometimes called attrition rate), in its broadest sense, is a measure of the number of individuals or items moving out of a collective group over a specific period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are neural networks?\n",
    "Neural networks, commonly known as Artificial Neural Networks (ANN) are quite a simulation of human brain functionality in machine learning (ML) problems. ANNs shall be noted not as a solution for all the problems that arise, but would provide better results with many other techniques altogether for various ML tasks. Most common use of ANNs are clustering and classification, which can be used for regression tasks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin\n",
    "So, In our dataset we would be dealing with Churn Modeling i.e. we would be writing a Artificial Neural Network to find out reasons as to why and which customers are actually leaving the bank and their dependencies on one another. This is a classification problem 0-1 classification(1 if customer Leaves and 0 if customer stays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s import our python libraries and our dataset creating X for our independent variable and y for our dependent variable\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Bank_Churn.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the features we can see that row no.,name will have no relation with a customer with leaving the bank.\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let’s start from the data:\n",
    "The dependent variable (Exited), the value that we are going to predict, will be the exit of the customer from the bank (binary variable 0 if the customer stays and 1 if the client exit).\n",
    "\n",
    "The independent variables will be\n",
    "\n",
    "Credit Score: reliability of the customer\n",
    "Geography: where is the customer from\n",
    "Gender: Male or Female\n",
    "Age\n",
    "Tenure: number of years of customer history in the company\n",
    "Balance: the money in the bank account\n",
    "Number of products of the customer in the bank\n",
    "Credit Card: if the customer has or not the CC\n",
    "Active: if the customer is active or not\n",
    "Estimated Salary: estimation of salary based on the entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have quality data and selected the right target, we will prepare the data for the model. In general we do not need big data to create a model, but if our variables are significant, modelling with hundreds of data could work. We need always to test test our models to check if everything works correctly. Let’s say for our example to work with 10.000 rows dataset We will split our entire dataset in 2 parts. The bigger part, that will be 80% of data, will be used for the training of the model, while the remaining 20% will be used to test the model and have its accuracy.\n",
    "\n",
    "But our model needs in input numerical data, so, we need to encode categorical data into numerical data. In This case we have Geography (France, Spain and Germany) and Gender (Male and Female). For Geography we will have 0,1,2 instead of France, Spain and Gemany and 0,1 instead of Gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating Dummy variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 6.1900000e+02, ..., 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0134888e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.0800000e+02, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 1.1254258e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 5.0200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 1.1393157e+05],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.0900000e+02, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 4.2085580e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 7.7200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 9.2888520e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.9200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 3.8190780e+04]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training it is important to apply feature scaling to data. It is an important step because it standardize the data and give them the same scale also for a faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to create our Artificial Neural Network and for doing this we will need to import the Keras library\n",
    "\n",
    "Listing out the steps involved in training the ANN with Stochastic Gradient Descent:-\n",
    "\n",
    "1)Randomly initialize the weights to small numbers close to 0(But not 0)\n",
    "2)Input the 1st observation of your dataset in the Input Layer, each Feature in one Input Node\n",
    "3)Forward-Propagation from Left to Right, the neurons are activated in a way that the impact of each neuron’s activation\n",
    "is limited by the weights.Propagate the activations until getting the predicted result y.\n",
    "4)Compare the predicted result with the actual result. Measure the generated error.\n",
    "5)Back-Propagation: From Right to Left, Error is back propagated.Update the weights according to how much they are\n",
    "responsible for the error.The Learning Rate tells us by how much such we update the weights.\n",
    "6)Repeat Steps 1 to 5 and update the weights after each observation(Reinforcement Learning).\n",
    "Or: Repeat Steps 1 to 5 but update the weights only after a batch of observations(Batch Learning) \n",
    "7)When the whole training set is passed through the ANN.That completes an Epoch. Redo more Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to define the number of hidden layers, the number of nodes for each hidden layers. We know that our input layers will have as many nodes as our independent variables (in our example we have 11 independent variables) and as output layers we will have our y, so 1 dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Dense function A tip for the definition of the number of nodes of hidden layers, based on experiments, is to choose the average between the input and the output layers. In this case we will have 6 nodes (11+1)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.4862 - acc: 0.7956\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4276 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.4220 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.4183 - acc: 0.8222\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4162 - acc: 0.8255\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4137 - acc: 0.8287\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4128 - acc: 0.8309\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4115 - acc: 0.8319\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4104 - acc: 0.8340\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4095 - acc: 0.8320\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4085 - acc: 0.8332\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4078 - acc: 0.8345\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4074 - acc: 0.8335\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4061 - acc: 0.8352\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4062 - acc: 0.8334\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.4060 - acc: 0.8334\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4055 - acc: 0.8340\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4049 - acc: 0.8341\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.4048 - acc: 0.8350\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.4049 - acc: 0.8339\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.4043 - acc: 0.8347\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4040 - acc: 0.8360\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4037 - acc: 0.8365\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4033 - acc: 0.8366\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4037 - acc: 0.8344\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4033 - acc: 0.8342\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4028 - acc: 0.8350\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4029 - acc: 0.8332\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4026 - acc: 0.8347\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4026 - acc: 0.8346\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4025 - acc: 0.8345\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4019 - acc: 0.8340\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4021 - acc: 0.8342\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4019 - acc: 0.8366\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4016 - acc: 0.8341\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.4022 - acc: 0.8342\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4017 - acc: 0.8350\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.4012 - acc: 0.8355\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.4016 - acc: 0.8335\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4010 - acc: 0.8359\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 0.4013 - acc: 0.8347\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 0.4015 - acc: 0.8340\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.4013 - acc: 0.8345\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.4011 - acc: 0.8366\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4012 - acc: 0.8356\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4011 - acc: 0.8350\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4006 - acc: 0.8336\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4010 - acc: 0.8342\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4009 - acc: 0.8356\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4010 - acc: 0.8350\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.4005 - acc: 0.8339\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4006 - acc: 0.8352\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.4005 - acc: 0.8355\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4006 - acc: 0.8369\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.4006 - acc: 0.8351\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.4002 - acc: 0.8350\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.4009 - acc: 0.8352\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.3998 - acc: 0.8350\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.4008 - acc: 0.8341\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4005 - acc: 0.8349\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.3996 - acc: 0.834 - 1s 113us/step - loss: 0.4005 - acc: 0.8346\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4004 - acc: 0.8357\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4000 - acc: 0.8349\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.4006 - acc: 0.8335\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.4003 - acc: 0.8354\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 0.4003 - acc: 0.8347\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 0.4001 - acc: 0.8342\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.4004 - acc: 0.8337\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 0.3999 - acc: 0.8355\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4001 - acc: 0.8340\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.4000 - acc: 0.8342\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.3993 - acc: 0.8342\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 0.4002 - acc: 0.8339\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.4001 - acc: 0.8346\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 0.4000 - acc: 0.8356\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 0.4000 - acc: 0.8345\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 0.3999 - acc: 0.8342\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.3999 - acc: 0.8349\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.3998 - acc: 0.8352\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 0.3998 - acc: 0.8342\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.3996 - acc: 0.8347\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.3999 - acc: 0.8340\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.3997 - acc: 0.8346\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4001 - acc: 0.8355\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.3998 - acc: 0.8332\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.4001 - acc: 0.8351\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.3994 - acc: 0.8345\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.3994 - acc: 0.8341\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.3995 - acc: 0.8361\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.4000 - acc: 0.8350\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.3999 - acc: 0.8346\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.3996 - acc: 0.8350\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.3995 - acc: 0.8350\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 0.3993 - acc: 0.8351\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3999 - acc: 0.8340\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3996 - acc: 0.8356\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3999 - acc: 0.8344\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.3997 - acc: 0.8347\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.3997 - acc: 0.8341\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.3998 - acc: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa0e8d42ba8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions and evaluating the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1547,   48],\n",
       "       [ 261,  144]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8455"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy = (1547+144)/(48+261+1547+144)\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy of the Neural Network Model is 84.5% which is decent enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
